---
layout: post
title: Comparing activation function ReLU vs Mish
subtitle: Comparing ReLU vs Mish activation function on classification accuracy of MNIST dataset. 
bigimg: "https://raw.githubusercontent.com/digantamisra98/Mish/master/Observations/logo_transparent.png"
tags: [CNN,Mish,vision]
---

# Comparing activation function ReLU vs Mish

*  Mish is Self Regularized Non-Monotonic Activation Function
*  ReLU ( Rectified Linear Unit )

