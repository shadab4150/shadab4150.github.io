---
layout: post
title: Comparing activation function ReLU vs Mish
subtitle: Comparing ReLU vs Mish activation function on classification accuracy of MNIST dataset. 
bigimg: "/img/actfuncdark.png"
tags: [CNN,Mish,vision]
---

# Comparing activation function ReLU vs Mish

*  Mish is Self Regularized Non-Monotonic Activation Function
*  ReLU ( Rectified Linear Unit )

