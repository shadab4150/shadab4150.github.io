---
layout: post
title: Comparing activation function ReLU vs Mish
subtitle: Comparing ReLU vs Mish activation function on classification accuracy of MNIST dataset. 
bigimg: "/img/msh1.jpg"
tags: [CNN,Mish,ReLU]
---

# Comparing activation function ReLU vs Mish

##  **ReLU** ( Rectified Linear Unit )

* ReLU is a type of activation function. Mathematically, it is defined as ***y = max(0, x).***

<center>Visually, it looks like the following:</center>


<center><img src="https://miro.medium.com/max/1026/1*DfMRHwxY1gyyDmrIAd-gjQ.png"></center>


*  Mish is Self Regularized Non-Monotonic Activation Function


